{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Lab\n",
    "\n",
    "\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_df = pd.read_csv('WH Report_preprocessed.csv')\n",
    "report_df.pivot(index=['Name','Continent'], columns='year', values=['population', 'Life_Ladder',\n",
    "       'Log_GDP_per_capita', 'Social_support',\n",
    "       'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n",
    "       'Generosity', 'Perceptions_of_corruption', 'Positive_affect',\n",
    "       'Negative_affect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_pdf= report_df.pivot(index=['Name'], columns='year', values=['population', 'Life_Ladder',\n",
    "       'Log_GDP_per_capita', 'Social_support',\n",
    "       'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n",
    "       'Generosity', 'Perceptions_of_corruption', 'Positive_affect',\n",
    "       'Negative_affect'])\n",
    "report_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = report_pdf[[ 'Life_Ladder',\n",
    "       'Log_GDP_per_capita', 'Social_support',\n",
    "       'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n",
    "       'Generosity', 'Perceptions_of_corruption', 'Positive_affect',\n",
    "       'Negative_affect']]\n",
    "Xs = (Xs - Xs.min())/(Xs.max()-Xs.min())\n",
    "Xs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clusteribility\n",
    "\n",
    "## Hopkins Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins(df,m):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from random import sample\n",
    "    from pandas import DataFrame\n",
    "    from numpy import random\n",
    "\n",
    "    d = len(df.columns) # columns\n",
    "    n = len(df) # rows\n",
    "    \n",
    "    df = (df - df.min())/(df.max()-df.min()) *2 -1\n",
    "    df = df / df.std()\n",
    "    \n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=2).fit(df)\n",
    "\n",
    "    rand_df = DataFrame(random.rand(m,d),index = range(0,m),columns =df.columns )\n",
    "    rand_df = rand_df*2-1\n",
    "    rand_df = rand_df * df.abs().max()\n",
    "\n",
    "    ujd = []\n",
    "    wjd = []\n",
    "\n",
    "    for j in range(0, m):\n",
    "        u_dist, _ = knn.kneighbors([rand_df.iloc[j]])\n",
    "        ujd.append(u_dist[0][0])\n",
    "\n",
    "        w_dist, _ = knn.kneighbors(df.sample(1))\n",
    "        wjd.append(w_dist[0][1])\n",
    "\n",
    "    return(sum(ujd) / (sum(ujd) + sum(wjd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "hopkins(Xs,m)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7).fit(Xs)\n",
    "\n",
    "# Cluster membership\n",
    "memb = pd.Series(kmeans.labels_, index=Xs.index)\n",
    "for key, item in memb.groupby(memb):\n",
    "    print(key, ': ', ', '.join(item.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countpairs(Clustering1,Clustering2):\n",
    "    from pandas import Series\n",
    "    \n",
    "    output = Series(0,index=['N00','N01','N10','N11'])\n",
    "\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,i):\n",
    "            if(i!=j):\n",
    "\n",
    "                c1_same = False\n",
    "                c2_same = False\n",
    "                c1_Not_same = False\n",
    "                c2_Not_same = False\n",
    "\n",
    "                if(Clustering1[i]==Clustering1[j]):\n",
    "                    c1_same=True\n",
    "                else:\n",
    "                    c1_Not_same=True\n",
    "                if(Clustering2[i]==Clustering2[j]):\n",
    "                    c2_same=True\n",
    "                else:\n",
    "                    c2_Not_same = True\n",
    "\n",
    "                if(c1_same & c2_same):\n",
    "                    output.N11 = output.N11 +1 \n",
    "                if(c1_Not_same & c2_Not_same):\n",
    "                    output.N00 = output.N00 +1\n",
    "                if(c1_same & c2_Not_same):\n",
    "                    output.N10 = output.N10 +1\n",
    "                if(c1_Not_same & c2_same):\n",
    "                    output.N01 = output.N01 +1\n",
    "\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fowlkesâ€“Mallows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fowlkes_mallows(Clustering1,Clustering2):\n",
    "    \n",
    "    from numpy import sqrt\n",
    "    from pandas import Series\n",
    "    \n",
    "    p = countpairs(Clustering1,Clustering2)\n",
    "    \n",
    "    return(p.N11/((p.N11+p.N01)+(p.N11+p.N10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure K-Means consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7)\n",
    "Clustering1 = kmeans.fit(Xs).labels_\n",
    "Clustering2 = kmeans.fit(Xs).labels_\n",
    "\n",
    "print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    Clustering1 = kmeans.fit(Xs).labels_\n",
    "    Clustering2 = kmeans.fit(Xs).labels_\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the number of clusters using SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SSE_results = pd.DataFrame(0.0, index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SSE_results.index:\n",
    "    for col in SSE_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(Xs)\n",
    "        SSE_results.at[n_cluster,col] = algort.inertia_ \n",
    "        # Inertia: Sum of distances of samples to their closest cluster center\n",
    "\n",
    "SSE_results['Mean'] = SSE_results[repetitions].mean(axis=1)\n",
    "SSE_results['Var'] = SSE_results[repetitions].var(axis=1)\n",
    "SSE_results.sort_values('Mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(SSE_results.Mean).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the number of clusters using Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(Xs)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(Xs,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SIL_results.Mean.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Cluster membership\n",
    "for i in range(0,20):\n",
    "    Clustering1 = kmeans.fit(Xs).labels_\n",
    "    Clustering2 = kmeans.fit(Xs).labels_\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4).fit(Xs)\n",
    "memb =  pd.Series(kmeans.labels_, index=Xs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# centroid Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = ['Cluster {}'.format(i) for i in range(4)]\n",
    "Centroids_orig = pd.DataFrame(0.0, index = clusters,\n",
    "                        columns = report_pdf.columns)\n",
    "\n",
    "Centroids_std = pd.DataFrame(0.0, index =  clusters,\n",
    "                        columns = Xs.columns)\n",
    "for i in range(4):\n",
    "    BM = memb==i\n",
    "    Centroids_orig.iloc[i] = report_pdf[BM].median(axis=0)\n",
    "    Centroids_std.iloc[i] = Xs[BM].mean(axis=0)\n",
    "    \n",
    "Centroids_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,5))\n",
    "sns.heatmap(Centroids_std, linewidths=.5, annot=True, \n",
    "                    cmap='Purples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replace_dic = {0:'unhappy but generous',\n",
    "               1:'generously happy but crime-ridden',\n",
    "               2:'happy but crime-ridden',\n",
    "               3:'Very happy'}\n",
    "report_pdf['Cluster_noPreprocess'] = memb.replace(replace_dic)\n",
    "report_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = PCA()\n",
    "pcs.fit(Xs)\n",
    "\n",
    "\n",
    "pcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n",
    "                           'Proportion of variance': pcs.explained_variance_ratio_,\n",
    "                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_),\n",
    "                             'Variance Explanation Ratio': pcs.explained_variance_ratio_,\n",
    "                             'Cumulative Ratio' : np.cumsum(pcs.explained_variance_ratio_) })\n",
    "\n",
    "pcsSummary_df = pcsSummary_df.transpose()\n",
    "pcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(Xs.columns) + 1)]\n",
    "pcsSummary_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(pcs.fit_transform(Xs), index = report_pdf.index,\n",
    "                      columns=[f'PC{i}' for i in range(1, len(Xs.columns) + 1)])\n",
    "scores = scores[[f'PC{i}' for i in range(1, 9)]]\n",
    "\n",
    "scores.plot.scatter(x='PC1', y='PC2',c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores.plot.scatter(x='PC1', y='PC2',c='PC3', cmap='YlOrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the  number of clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SSE_results = pd.DataFrame(0.0, index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SSE_results.index:\n",
    "    for col in SSE_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(scores)\n",
    "        SSE_results.at[n_cluster,col] = algort.inertia_ \n",
    "        # Inertia: Sum of distances of samples to their closest cluster center\n",
    "\n",
    "SSE_results['Mean'] = SSE_results[repetitions].mean(axis=1)\n",
    "SSE_results['Var'] = SSE_results[repetitions].var(axis=1)\n",
    "SSE_results.sort_values('Mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(SSE_results.Mean).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(scores)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(scores,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SIL_results.Mean.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Cluster membership\n",
    "for i in range(0,20):\n",
    "    Clustering1 = kmeans.fit(scores).labels_\n",
    "    Clustering2 = kmeans.fit(scores).labels_\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4).fit(scores)\n",
    "memb =  pd.Series(kmeans.labels_, index=Xs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centroid Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters = ['Cluster {}'.format(i) for i in range(4)]\n",
    "Centroids_orig = pd.DataFrame(0.0, index = clusters,\n",
    "                        columns = report_pdf.columns)\n",
    "\n",
    "Centroids_pca = pd.DataFrame(0.0, index =  clusters,\n",
    "                        columns = scores.columns)\n",
    "for i in range(4):\n",
    "    BM = memb==i\n",
    "    Centroids_orig.iloc[i] = report_pdf[BM].median(axis=0)\n",
    "    Centroids_pca.iloc[i] = scores[BM].mean(axis=0)\n",
    "    \n",
    "Centroids_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Centroids_pca = (Centroids_pca - Centroids_pca.min())/(Centroids_pca.max()-Centroids_pca.min())\n",
    "\n",
    "sns.heatmap(Centroids_pca, linewidths=.5, annot=True, \n",
    "                    cmap='Purples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_pdf['Cluster_pca'] = memb\n",
    "report_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_tbl = pd.crosstab(report_pdf.Cluster_noPreprocess, report_pdf.Cluster_pca)\n",
    "probablity_tbl = contingency_tbl/ contingency_tbl.sum()\n",
    "sns.heatmap(probablity_tbl, annot=True, center=0.5 ,cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM = report_df.Name =='United States'\n",
    "wdf = report_df[BM][['year','Life_Ladder']]\n",
    "wdf.reset_index(drop=True,inplace=True)\n",
    "wdf.drop(columns = ['year'],inplace=True)\n",
    "wdf['integer'] = range(len(wdf))\n",
    "wdf['ones'] = 1\n",
    "wdf.Life_Ladder.plot()\n",
    "plt.title('USA Life Ladder')\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "lm = LinearRegression()\n",
    "lm.fit(wdf.drop(columns=['Life_Ladder']), wdf.Life_Ladder)\n",
    "\n",
    "b = lm.intercept_\n",
    "a = lm.coef_[0]\n",
    "\n",
    "X = wdf.integer\n",
    "y = b + a*X\n",
    "\n",
    "plt.plot(X,y,label = 'Fitted regression')\n",
    "plt.legend()\n",
    "\n",
    "print('Feature one: Mean = {}'.format(wdf.Life_Ladder.mean()))\n",
    "print('Feature two: Slope = {}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a placeholder for preprocessing\n",
    "columns = ['Life_Ladder', 'Log_GDP_per_capita', 'Social_support',\n",
    "           'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n",
    "           'Generosity', 'Perceptions_of_corruption', 'Positive_affect',\n",
    "           'Negative_affect']\n",
    "features = ['Mean','Slope']\n",
    "\n",
    "my_column = pd.MultiIndex.from_product([features,columns],\n",
    "                                     names=('features','columns'))\n",
    "\n",
    "preprocess_df = pd.DataFrame(index = report_pdf.index,\n",
    "                       columns=my_column)\n",
    "\n",
    "preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries = report_df.Name.unique()\n",
    "columns = ['Life_Ladder', 'Log_GDP_per_capita', 'Social_support',\n",
    "           'Healthy_life_expectancy_at_birth', 'Freedom_to_make_life_choices',\n",
    "           'Generosity', 'Perceptions_of_corruption', 'Positive_affect',\n",
    "           'Negative_affect']\n",
    "\n",
    "for ct in Countries:\n",
    "    for col in columns:\n",
    "        BM = report_df.Name ==ct\n",
    "               \n",
    "        wdf = pd.DataFrame(report_df[BM][col])\n",
    "        wdf.reset_index(drop=True,inplace=True)\n",
    "        wdf['integer'] = range(len(wdf))\n",
    "        wdf['ones'] = 1\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(wdf.drop(columns=[col]), wdf[col])\n",
    "        a = lm.coef_[0]\n",
    "        preprocess_df.at[ct,('Slope',col)]=a\n",
    "        preprocess_df.at[ct,('Mean',col)]=wdf[col].mean()\n",
    "preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preprocess_df.loc['United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = (preprocess_df - preprocess_df.min())/(preprocess_df.max()-preprocess_df.min())\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SSE_results = pd.DataFrame(0.0, index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SSE_results.index:\n",
    "    for col in SSE_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(Xs)\n",
    "        SSE_results.at[n_cluster,col] = algort.inertia_ \n",
    "        # Inertia: Sum of distances of samples to their closest cluster center\n",
    "\n",
    "SSE_results['Mean'] = SSE_results[repetitions].mean(axis=1)\n",
    "SSE_results['Var'] = SSE_results[repetitions].var(axis=1)\n",
    "SSE_results.sort_values('Mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SSE_results.Mean).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(Xs)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(Xs,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SIL_results.Mean.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Cluster membership\n",
    "for i in range(0,20):\n",
    "    Clustering1 = kmeans.fit(Xs).labels_\n",
    "    Clustering2 = kmeans.fit(Xs).labels_\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4).fit(Xs)\n",
    "memb =  pd.Series(kmeans.labels_, index=Xs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centroid Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = ['Cluster {}'.format(i) for i in range(4)]\n",
    "Centroids_orig = pd.DataFrame(0.0, index = clusters,\n",
    "                        columns = preprocess_df.columns)\n",
    "\n",
    "Centroids_std = pd.DataFrame(0.0, index =  clusters,\n",
    "                        columns = Xs.columns)\n",
    "for i in range(4):\n",
    "    BM = memb==i\n",
    "    Centroids_orig.iloc[i] = preprocess_df[BM].median(axis=0)\n",
    "    Centroids_std.iloc[i] = Xs[BM].mean(axis=0)\n",
    "    \n",
    "Centroids_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.heatmap(Centroids_std, linewidths=.5, annot=True, \n",
    "                    cmap='Purples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dic = {2:'unhappy but generous',\n",
    "               1:'generously happy but crime-ridden',\n",
    "               0:'happy but crime-ridden',\n",
    "               3:'Very happy'}\n",
    "report_pdf['Cluster_functional'] = memb.replace(replace_dic)\n",
    "report_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_tbl = pd.crosstab(report_pdf.Cluster_noPreprocess, report_pdf.Cluster_functional)\n",
    "probablity_tbl = contingency_tbl/ contingency_tbl.sum()\n",
    "sns.heatmap(probablity_tbl, annot=True, center=0.5 ,cmap=\"Greys\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
