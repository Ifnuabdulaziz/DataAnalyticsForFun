{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Lab\n",
    "\n",
    "\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_df = pd.read_csv('WH Report_preprocessed.csv')\n",
    "BM = report_df.year ==2019\n",
    "report_df = pd.DataFrame(report_df[BM]).reset_index(drop=True)\n",
    "report_df.drop(columns=['year'],inplace=True)\n",
    "report_df.set_index('Name',inplace=True)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Select_columns = ['Life_Ladder', 'Log_GDP_per_capita',\n",
    "       'Social_support', 'Healthy_life_expectancy_at_birth',\n",
    "       'Freedom_to_make_life_choices', 'Generosity',\n",
    "       'Perceptions_of_corruption', 'Positive_affect', 'Negative_affect']\n",
    "Xs = pd.DataFrame(report_df[Select_columns])\n",
    "\n",
    "Xs = (Xs-Xs.min())/(Xs.max()-Xs.min())\n",
    "Xs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clusteribility\n",
    "\n",
    "## Hopkins Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins(df,m):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from random import sample\n",
    "    from pandas import DataFrame\n",
    "    from numpy import random\n",
    "\n",
    "    d = len(df.columns) # columns\n",
    "    n = len(df) # rows\n",
    "    \n",
    "    df = (df - df.min())/(df.max()-df.min()) *2 -1\n",
    "    df = df / df.std()\n",
    "    \n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=2).fit(df)\n",
    "\n",
    "    rand_df = DataFrame(random.rand(m,d),index = range(0,m),columns =df.columns )\n",
    "    rand_df = rand_df*2-1\n",
    "    rand_df = rand_df * df.abs().max()\n",
    "\n",
    "    ujd = []\n",
    "    wjd = []\n",
    "\n",
    "    for j in range(0, m):\n",
    "        u_dist, _ = knn.kneighbors([rand_df.iloc[j]])\n",
    "        ujd.append(u_dist[0][0])\n",
    "\n",
    "        w_dist, _ = knn.kneighbors(df.sample(1))\n",
    "        wjd.append(w_dist[0][1])\n",
    "\n",
    "    return(sum(ujd) / (sum(ujd) + sum(wjd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "hopkins(Xs,m)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print(hopkins(Xs,m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4).fit(Xs)\n",
    "\n",
    "# Cluster membership\n",
    "memb = pd.Series(kmeans.labels_, index=Xs.index)\n",
    "for key, item in memb.groupby(memb):\n",
    "    print(key, ': ', ', '.join(item.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countpairs(Clustering1,Clustering2):\n",
    "    from pandas import Series\n",
    "    \n",
    "    output = Series(0,index=['N00','N01','N10','N11'])\n",
    "\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,i):\n",
    "            if(i!=j):\n",
    "\n",
    "                c1_same = False\n",
    "                c2_same = False\n",
    "                c1_Not_same = False\n",
    "                c2_Not_same = False\n",
    "\n",
    "                if(Clustering1[i]==Clustering1[j]):\n",
    "                    c1_same=True\n",
    "                else:\n",
    "                    c1_Not_same=True\n",
    "                if(Clustering2[i]==Clustering2[j]):\n",
    "                    c2_same=True\n",
    "                else:\n",
    "                    c2_Not_same = True\n",
    "\n",
    "                if(c1_same & c2_same):\n",
    "                    output.N11 = output.N11 +1 \n",
    "                if(c1_Not_same & c2_Not_same):\n",
    "                    output.N00 = output.N00 +1\n",
    "                if(c1_same & c2_Not_same):\n",
    "                    output.N10 = output.N10 +1\n",
    "                if(c1_Not_same & c2_same):\n",
    "                    output.N01 = output.N01 +1\n",
    "\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering1 = np.random.randint(1,4,len(Xs))\n",
    "Clustering2 = np.random.randint(1,20,len(Xs))\n",
    "\n",
    "countpairs(Clustering1,Clustering2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fowlkesâ€“Mallows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fowlkes_mallows(Clustering1,Clustering2):\n",
    "    \n",
    "    from numpy import sqrt\n",
    "    from pandas import Series\n",
    "    \n",
    "    p = countpairs(Clustering1,Clustering2)\n",
    "    \n",
    "    return(p.N11/((p.N11+p.N01)+(p.N11+p.N10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering1 = np.random.randint(1,4,len(Xs))\n",
    "Clustering2 = np.random.randint(1,4,len(Xs))\n",
    "\n",
    "fowlkes_mallows(Clustering1,Clustering2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    Clustering1 = np.random.randint(1,4,22)\n",
    "    Clustering2 = np.random.randint(1,4,22)\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering1 = np.random.randint(1,4,len(Xs))\n",
    "\n",
    "print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure K-Means consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "Clustering1 = kmeans.fit(Xs).labels_\n",
    "Clustering2 = kmeans.fit(Xs).labels_\n",
    "\n",
    "print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    Clustering1 = kmeans.fit(Xs).labels_\n",
    "    Clustering2 = kmeans.fit(Xs).labels_\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the number of clusters using SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SSE_results = pd.DataFrame(0.0, index = range(2,15), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SSE_results.index:\n",
    "    for col in SSE_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(Xs)\n",
    "        SSE_results.at[n_cluster,col] = algort.inertia_ \n",
    "        # Inertia: Sum of distances of samples to their closest cluster center\n",
    "\n",
    "SSE_results['Mean'] = SSE_results[repetitions].mean(axis=1)\n",
    "SSE_results['Var'] = SSE_results[repetitions].var(axis=1)\n",
    "SSE_results.sort_values('Mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(SSE_results.Mean).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the number of clusters using Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "algort = KMeans(n_clusters=3).fit(Xs)\n",
    "silhouette_score(Xs,algort.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,25), \n",
    "                       columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster).fit(Xs)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(Xs,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIL_results.Mean.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    Clustering1 = KMeans(n_clusters=3).fit(Xs).labels_\n",
    "    Clustering2 = KMeans(n_clusters=3).fit(Xs).labels_\n",
    "    \n",
    "    print('fowlkes_mallows: {}'.format(fowlkes_mallows(Clustering1,Clustering2)))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we decide, k=3 is the best for this data. How do we know the output of which run should we us?\n",
    "\n",
    "We run the algorithms for m times, and the run that leads to the highest similarity of patterns to the rest of the runs will be selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=10\n",
    "\n",
    "Clusterings =[]\n",
    "\n",
    "for i in range(0,m):\n",
    "    algort = KMeans(n_clusters=3, random_state=i).fit(Xs)\n",
    "    Clusterings.append(algort.labels_)\n",
    "\n",
    "Sim_Matrix = pd.DataFrame(0.0,index= ['Clustering{}'.format(i) for i in range(1,m+1)], \n",
    "                          columns=['Clustering{}'.format(i) for i in range(1,m+1)])\n",
    "for i, inx in enumerate(Sim_Matrix.index):\n",
    "    for j, col in enumerate(Sim_Matrix.columns):\n",
    "        Sim_Matrix.at[inx,col] = fowlkes_mallows(Clusterings[i],Clusterings[j])\n",
    "\n",
    "Sim_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.heatmap(Sim_Matrix, linewidths=.5, annot=True, \n",
    "                    cmap='Greens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memb = pd.Series(Clusterings[2], index=Xs.index)\n",
    "for key, item in memb.groupby(memb):\n",
    "    print(key, ': ', ', '.join(item.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# centroid Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = ['Cluster {}'.format(i) for i in range(3)]\n",
    "Centroids_orig = pd.DataFrame(0.0, index = clusters,\n",
    "                        columns = report_df.columns)\n",
    "\n",
    "Centroids_std = pd.DataFrame(0.0, index =  clusters,\n",
    "                        columns = Xs.columns)\n",
    "for i in range(3):\n",
    "    BM = memb==i\n",
    "    Centroids_orig.iloc[i] = report_df[BM].median(axis=0)\n",
    "    Centroids_std.iloc[i] = Xs[BM].mean(axis=0)\n",
    "    \n",
    "Centroids_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Centroids_std, linewidths=.5, annot=True, \n",
    "                    cmap='Purples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replace_dic = {0:'Happy and crime-ridden',\n",
    "               1:'Unhappy and crime-ridden',\n",
    "               2:'Very happy'}\n",
    "report_df['Cluster'] = memb.replace(replace_dic)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More analysis \n",
    "Investigate the relationship between the attribute cluster and the attributes we didn't used for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_tbl = pd.crosstab(report_df.Cluster, report_df.Continent)\n",
    "probablity_tbl = contingency_tbl/ contingency_tbl.sum()\n",
    "sns.heatmap(probablity_tbl, annot=True, center=0.5 ,cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_tbl = pd.crosstab(report_df.Continent, report_df.Cluster)\n",
    "probablity_tbl = contingency_tbl/ contingency_tbl.sum()\n",
    "sns.heatmap(probablity_tbl, annot=True, center=0.5 ,cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_discretized = pd.cut(report_df.population, bins = 10)\n",
    "contingency_tbl = pd.crosstab(report_df.Cluster,pop_discretized)\n",
    "probablity_tbl = contingency_tbl/ contingency_tbl.sum()\n",
    "sns.heatmap(probablity_tbl, annot=True, center=0.5 ,cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(report_df.population,\n",
    "           memb.apply(lambda v: v+np.random.random()/1.8),\n",
    "           marker='.')\n",
    "plt.yticks([0,1,2],['Happy and crime-ridden',\n",
    "                     'Unhappy and crime-ridden',\n",
    "                     'Very happy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
